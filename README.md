# KDSH 2026 - Track B: BDH-Driven Continuous Narrative Reasoning

**Team**: Hackathon-nikita  
**Track**: B - Long-form Narrative Consistency Classification  
**Date**: January 2026

---

## ğŸ¯ Solution Overview

This solution uses the **Baby Dragon Hatchling (BDH)** architecture with **Pathway framework** to determine if a character's hypothetical backstory is consistent with a long-form novel (100k+ words).

### Key Features
- âœ… BDH architecture with stateful attention and persistent memory
- âœ… Pathway framework for document ingestion and vector storage
- âœ… Handles novels up to 400k+ words
- âœ… Binary classification: Consistent (1) vs Inconsistent (0)
- âœ… CPU-optimized for Mac training

---

## ğŸ“Š Dataset

- **Training**: 80 examples (51 consistent, 29 inconsistent)
- **Test**: 60 examples
- **Novels**: "In Search of the Castaways" (138k words) + "The Count of Monte Cristo" (464k words)

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Prepare Data

The training data should be in `data/raw/` with format:
```
data/raw/
â”œâ”€â”€ {id}_novel.txt
â”œâ”€â”€ {id}_backstory.txt
â””â”€â”€ labels.json
```

### 3. Train Model

```bash
python main.py train --config config.yaml
```

**Training Time**: ~2-2.5 hours on CPU (5 epochs, 80 examples)

### 4. Generate Predictions

```bash
python main.py infer \
  --model models/best_model.pt \
  --test-data data/test/ \
  --output results/predictions.csv
```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                 # Entry point (train/infer/test)
â”œâ”€â”€ config.yaml            # Configuration
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bdh_model.py       # BDH architecture
â”‚   â”œâ”€â”€ data_ingestion.py  # Pathway data loading
â”‚   â”œâ”€â”€ consistency_classifier.py
â”‚   â”œâ”€â”€ train.py           # Training pipeline
â”‚   â”œâ”€â”€ inference.py       # Inference pipeline
â”‚   â””â”€â”€ utils.py           # Utilities
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model.pt      # Trained model
â”œâ”€â”€ results/
â”‚   â””â”€â”€ predictions.csv    # Final predictions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Training data
â”‚   â””â”€â”€ test/              # Test data
â””â”€â”€ report/
    â””â”€â”€ REPORT.md          # Technical report
```

---

## ğŸ—ï¸ Architecture

### BDH Model
- **6 layers**, 384 hidden dim, 6 attention heads
- **Stateful attention** with persistent memory (256 tokens)
- **Sparse updates** for efficient long-context processing
- **Incremental chunking** (512 tokens/chunk, 256 overlap)

### Training
- **Optimizer**: AdamW (lr=2e-5, weight decay=0.01)
- **Scheduler**: Linear warmup (100 steps)
- **Batch size**: 2 with gradient accumulation (2 steps)
- **Early stopping**: Patience of 3 epochs

---

## ğŸ“ˆ Results

- **Training Accuracy**: ~XX% (see `logs/training.log`)
- **Validation Accuracy**: ~XX%
- **Test Predictions**: `results/predictions.csv`

---

## ğŸ”§ Configuration

Key settings in `config.yaml`:

```yaml
bdh:
  hidden_dim: 384
  num_layers: 6
  memory_size: 256

training:
  num_epochs: 5
  batch_size: 2
  learning_rate: 2e-5

computation:
  device: "cpu"
  num_workers: 0
```

For GPU training, change `device: "cuda"` and increase `batch_size`.

---

## ğŸ“ Output Format

Predictions are in CSV format:

```csv
Story ID,Prediction,Rationale
1,0,Backstory contradicts novel timeline
2,1,Backstory aligns with character development
```

- **Story ID**: Test example ID
- **Prediction**: 0 (Inconsistent) or 1 (Consistent)
- **Rationale**: Brief explanation (optional for Track B)

---

## ğŸ§ª Testing

```bash
# Test environment setup
python main.py test --config config.yaml
```

---

## ğŸ“š Dependencies

Main libraries:
- PyTorch 2.6+
- Transformers (Hugging Face)
- Pathway (vector store)
- Sentence-Transformers (embeddings)
- NumPy, Pandas, tqdm

See `requirements.txt` for full list.

---

## ğŸ“ Technical Details

### Chunking Strategy
Long novels are split into overlapping chunks:
- Chunk size: 512 tokens
- Overlap: 256 tokens
- Max novel length: 120,000 words

### Attention Mechanism
- Multi-head attention with stateful memory
- Sparse attention (threshold: 0.1)
- Selective state updates

### Data Augmentation
- Novels matched with multiple backstories
- Both consistent and inconsistent examples
- Character-focused backstories

---

## ğŸ‘¥ Team

**Hackathon-nikita**

---

## ğŸ“„ License

This project is submitted for KDSH 2026 Track B.

---

## ğŸ“ Support

For questions about this submission, refer to:
- `report/REPORT.md` - Detailed technical report
- `QUICKSTART.md` - Quick setup guide
- `NEXT_STEPS.md` - Development roadmap

---

**Submission Date**: January 2026  
**Track**: B - Long-form Narrative Consistency
# BDH-HACKATHON
